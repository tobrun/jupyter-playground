{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Text classifier\n",
    "Models lke DistilBERT are pretrained to predict masked words in a sequence of text. We can't use these language models directly for text classification. We have two options to train a model on our Twitter dataset:\n",
    " - Feature Extraction: usage of hidden states as features and train a classifier on them, without modifiying the pretrained model\n",
    " - Fine-tuning: train model end-to-end, which also updates the parameters of the pretrained model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup tokenizer (see previous notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "emotions = load_dataset(\"emotion\")\n",
    "\n",
    "# Applies tokenizer to a batch of examples, padding true adds examples with zeros and truncation true truncates examples to max context length.\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "emotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tranformers as feature extractors\n",
    "\n",
    "Using transformers as feature extractor is fairly simple. We freeze the body weights during training and use the hidde states as features for the classifier. We can quickly train a small or shallow model. Such a model could be a neural classification layer o a method that does not rely on gradients, such as a random forest. This method is especially convenient if GPUs are unavaillable, since the hidden states only need to be precomputed once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs tensor shape: torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModel.from_pretrained(model_ckpt).to(device)\n",
    "\n",
    "# The AutoModel converts the token encodings to embeddings, and then feeds them through the encoder stack to return the hidden states.\n",
    "\n",
    "text = \"this is a test\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "print(f\"inputs tensor shape: {inputs['input_ids'].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[-0.1565, -0.1862,  0.0528,  ..., -0.1188,  0.0662,  0.5470],\n",
       "         [-0.3575, -0.6484, -0.0618,  ..., -0.3040,  0.3508,  0.5221],\n",
       "         [-0.2772, -0.4459,  0.1818,  ..., -0.0948, -0.0076,  0.9958],\n",
       "         [-0.2841, -0.3917,  0.3753,  ..., -0.2151, -0.1173,  1.0526],\n",
       "         [ 0.2661, -0.5094, -0.3180,  ..., -0.4203,  0.0144, -0.2149],\n",
       "         [ 0.9441,  0.0112, -0.4714,  ...,  0.1439, -0.7288, -0.1619]]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have the encodings as a tensor, the final step is to place them on the same device as the model and pass the inputs as follows\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we used `torch.no_grad()` context manager to disable the automatic calc of the gradient. This is useful for inference since it reduces the mem footprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 768])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state.size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "768 vector is returend for each of the 6 input tokens.\n",
    "For classificaion tasks, it is common practicie to us the hdiden state associated with the [CLS] token as the input feature. Since the token appears at the start of each sequence, we can extract it by simply indexing into outputs.last_hidden_state as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state[:,0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text', 'label', 'input_ids', 'attention_mask', 'hidden_state']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now that we can do this for a single string, we can generalize it in a function\n",
    "# the only diff is that we place the hidden state on the cpu as a NumPy array\n",
    "def extract_hidden_states(batch):\n",
    "    inputs = {k: torch.tensor(v).to(device) for k, v in batch.items() if k in tokenizer.model_input_names}\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(**inputs).last_hidden_state\n",
    "    return {\"hidden_state\": last_hidden_states[:, 0].cpu().numpy()}\n",
    "\n",
    "\n",
    "# since out model expects tensor as inputs, the next thing is to convert the inputs_ids and attention_mask\n",
    "emotions_encoded.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "emotions_hidden = emotions_encoded.map(extract_hidden_states, batched=True)\n",
    "\n",
    "# the latter adds a new hidden_state column to the dataset\n",
    "emotions_hidden[\"train\"].column_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a feature matrix\n",
    "Now that we have the hdden states associated with each tweet, the next step is to train a classifier on them. For that we need a feature matrix.\n",
    "We will use the hidden states as input features and the labels as targets. We can easily create the corresponding arrays in the Scikit-learn format as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16000, 768), (2000, 768))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train = np.array(emotions_hidden[\"train\"][\"hidden_state\"])\n",
    "X_valid = np.array(emotions_hidden[\"validation\"][\"hidden_state\"])\n",
    "y_train = np.array(emotions_hidden[\"train\"][\"label\"])\n",
    "y_valid = np.array(emotions_hidden[\"validation\"][\"label\"])\n",
    "X_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'UMAP' from 'umap' (/home/nurbot/anaconda3/envs/hf/lib/python3.10/site-packages/umap/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/nurbot/ws/jupyter-playground/notebooks/hf_3_training_text_classifier.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/nurbot/ws/jupyter-playground/notebooks/hf_3_training_text_classifier.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# To check if the training set contains the emotions that we want to represent, we should visualize the results\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/nurbot/ws/jupyter-playground/notebooks/hf_3_training_text_classifier.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mumap\u001b[39;00m \u001b[39mimport\u001b[39;00m UMAP\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/nurbot/ws/jupyter-playground/notebooks/hf_3_training_text_classifier.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m MinMaxScaler\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/nurbot/ws/jupyter-playground/notebooks/hf_3_training_text_classifier.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mimport\u001b[39;00m DataFrame \u001b[39mas\u001b[39;00m pd\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'UMAP' from 'umap' (/home/nurbot/anaconda3/envs/hf/lib/python3.10/site-packages/umap/__init__.py)"
     ]
    }
   ],
   "source": [
    "# To check if the training set contains the emotions that we want to represent, we should visualize the results\n",
    "from umap import UMAP\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pandas import DataFrame as pd\n",
    "\n",
    "# Scale feature to [0,1] range\n",
    "X_scaled = MinMaxScaler().fit_transform(X_train)\n",
    "# Init UMAP and fit it to the scaled data\n",
    "mapper = UMAP(n_components=2, metric=\"cosine\").fit(X_scaled)\n",
    "# Create a DataFrame with the resulting 2D embedding and the corresponding labels\n",
    "df_emb = pd.DataFrame(mapper.embedding_, columns=[\"x\", \"y\"])\n",
    "df_emb[\"label\"] = y_train\n",
    "df_emb.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
